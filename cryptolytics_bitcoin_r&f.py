# -*- coding: utf-8 -*-
"""Cryptolytics_Bitcoin R&F.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A-PYFp_1djjhM-l4WBVQxunbm5lzbNdS

#B ITCOINALYTICS: the rise and fall of the market value of bitcoin over time


 I think that the rise and fall of bitcoin can be atributed to not only outlyer/ chance luck, but by also knowing when to invest in certain shares, and when to pull out based on market

I believe with this data set and "CYrtptolyts" data set we can predict a rise in cryptocurrency stock in the years 2025-2028 that will be similar to the rise seen in 2013-2016. this prediction was judged based on bitcoin in particular as a basis.

#import cell
"""

!pip install category_encoders
!pip install shap
!pip install pdpbox

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, mean_absolute_error, mean_squared_error, r2_score


import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.dummy import DummyRegressor


from category_encoders import OrdinalEncoder

#xgbosst import
from xgboost import XGBRegressor
#pdp import


from pdpbox.pdp import PDPIsolate, PDPInteract
from pdpbox.info_plots import TargetPlot, InteractTargetPlot



import shap

#shapeley values
from sklearn.inspection import PartialDependenceDisplay

#import graphs
import seaborn as sns
import matplotlib.pyplot as plt

"""#step 2
read in data and asssign it
"""

bit_rf= pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')

# from google.colab import drive
# drive.mount('/content/drive')

bit_rf.shape

bit_rf.head(1000000)



bit_rf.isnull().sum().sum()

#drop missing values
bit_rf.dropna(inplace= True)
#this goes in wrang. bfe return
#refer to sp2

bit_rf.info()

bit_rf.describe()

bit_rf.shape

bit_rf.plot(kind= 'scatter', x= 'Timestamp', y= 'Weighted_Price')

#start wrking on prediction after target: 'Weighted_Price'
#this is reg. bc we r solvinging for a num. prediction
#figuring out trgt dist.
#create plot from matplotlib
#bit_rf.plot(kind= 'scatter', x= 'Timestamp', y= 'Weighted_Price')
bit_rf['Weighted_Price'].plot(kind= 'hist')

# is it skewed?

bit_rf['Weighted_Price'].skew(axis=0, skipna= True)

"""#skewed to the right, or at least it was, now slightly skewed left

no log tranformation, we gotz da data, weightroom!

#Step 3: split data fr train tst
"""

#or'
# target= 'Weighted_Price'
# y= bit_rf[target]
# X= bit_rf.drop(columns= target)



y= bit_rf['Weighted_Price']
X= bit_rf.drop(columns= 'Weighted_Price')

#finding out whats in the target variable
y.nunique()

#random split fr test sets #member test size
#no stratify = true needed this is regression
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 42, test_size= 0.2)

"""#Log transformation of skewed target (for regression)"""

# Check if they the count is close btween the train and test of X
X_train.value_counts(normalize=True).round()

X_test.value_counts(normalize=True)

# Compare value counts of X_train and X_test
print("X_train value counts:\n", X_train.value_counts(normalize=True).round(3))
print("\nX_test value counts:\n", X_test.value_counts(normalize=True).round(3))

"""Make a regressor pipeline fr a prediction, after figuring out baseline"""

#check fr MAE, baseline reg, model?
y_pred= [y_train.mean()] * len(y_train) #/?/
print('Mean price:', y_train.mean())
print('Baseline MAE:', mean_absolute_error(y_train, y_pred))

"""#YAY!"""

#measure the mean abs error fr bseline BFRE PIPE
#create a quick RandomForrest model quick n ez
#model= RandomForestRegressor(random_state= 42)
#use standard scaler "look into sprint 6"
#no missin, no simpin
#create scaler and reg. in yr pipe
model = make_pipeline(
    StandardScaler(),
    RandomForestRegressor(random_state= 42)
)

model.fit(X_train, y_train)
y_pred= model.predict(X_test)
print('MAE:', mean_absolute_error(y_test, y_pred))

from google.colab import drive
drive.mount('/content/drive')

"""Grid search after pipeline"""

# grid / randomized search

"""lstm predictions are neccesary to move on further, itll be difficult!!!
you need ltsm to properly pred. time and recurring pattern changes over tme. find examples to hlp u fnd solutions, i.e.


CNN huge fr spatial analysis, facial rec., teslas, red bull?  they exel at spatial analysis, seem pretty cool. Not able to implement this unfortunatley fr here... but keep it noted

chatgbt implemetes concepts of lstm

#Step 4
Balance the data by transforming your target
"""

# log transform your target (training set)
y_train_log = np.log(y_train)
print(' the skew  of untransformed variable is:', y_train.skew(), 'skew of transformed is:', y_train_log.skew())

"""were goin untransformed here for our predictions, weve done that in the following cell"""

y_pred_log= model.predict(X_test)
print('MAE:', mean_absolute_error(y_test, y_pred_log))

if 'x' in X_train.columns:
    X_train = X_train.drop(columns='x')
if 'x' in X_test.columns:
    X_test = X_test.drop(columns='x')

boost= make_pipeline(
    StandardScaler(),
    XGBRegressor(random_state= 42, learning_rate= 0.3)

)
boost.fit(X_train, y_train)
y_pred_log= boost.predict(X_test)
print('MAE:', mean_absolute_error(y_test, y_pred_log))

"""better performance on random forest rather than xgboost?!!?
possible over fit.

MAE increased ... :(
"""

y_pred_log

"""#Step 5: Communicate restults of features"""

# get coefficiants from y_pred
importance = model.named_steps['randomforestregressor'].feature_importances_

# create a dataframe with the feature names and their corresponding coefficients
coef = pd.DataFrame({'Features': X_train.columns, 'importance': importance})
#sort the values and plot bar graph, make sure you label X.train columns
coef = coef.sort_values('importance', ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x='importance', y='Features', data=coef, palette='viridis')

#plot the feature importances
df_importances = pd.DataFrame({
    'feature': X_train.columns,
    'importance': boost.named_steps['xgbregressor'].feature_importances_
})
df_importances = df_importances.sort_values('importance', ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x='importance', y='feature', data=df_importances, palette='viridis')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.show()

""" well find out  what to do further after we see MAE and after a simpler model...
 #Tuning? find out next time on dragon ball Z!
"""

bit_rf.info()

"""#Step 6 Model interpretation
interperet the model with your fingings
create conclusions and a resulting visualiztion for them as well as a conclusion for the null*see abve*




Complete these tasks for your project, and document your work.

Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.
Make at least 1 partial dependence plot to explain your model.
Make at least 1 Shapley force plot to explain an individual prediction.
Share at least 1 visualization (of any type) on Slack!

Build a PDP plot explaining the prediction
"""

#select one feature
# DAT ONE ERROR drop x column
if 'x' in X_test.columns:
    X_test = X_test.drop(columns='x')
feature = 'Open'
# Create a PDPIsolate object for the feature
isolate = PDPIsolate(
    model=model,
    df=X_test,
    feature=feature,
    model_features=X_test.columns,
    feature_name=feature,
    n_classes= 0

)
#for matplotlib visualization
fig, axes= isolate.plot()

#pdp box use java to make it look cool, kinda like taekwondo vs kickboxing

fig

# !pip install -U shap scikit-learn

# eli 5 for plotting permutation importance

import eli5
from eli5.sklearn import PermutationImportance

eli5.show_weights(model, feature_names= X_test.columns.tolist())



#select one sample/observation
# Shapley Force Plot
# may not work well in local notebooks
# you may need to install ipywidgets
# !pip install ipywidgets


# DAT ONE ERROR (HE THAT SHALL NOT BE NAMED) drop x column
# if 'x' in X_train.columns:
#     X_train = X_train.drop(columns='x')
# # last row in the x.train set
# row_smp = X_train.tail(1)

# explainer = shap.TreeExplainer(boost)
# shap_values = explainer.shap_values(row_smp)

# #initializing Java Script
# shap.initjs()


# #force plot start with explainer
# shap.force_plot(
#     base_value=isolate.base_value,
#     shap_values=isolate.shap_values,
#     features=row_smp
# )

"""#notes"""

# # from os import plock
# #one feature

# feature = 'Open'

# isolate = pdp_isolate(
#     model=model,
#     dataset=X_train,
#     model_features=X_train.columns,
#     feature=feature
# )
# plot_pdp(isolate)

# #import partial
# from sklearn.inspection import PartialDependenceDisplay
# import matplotlib.pyplot as plt

# fig, ax = plt.subplots(1,1, figsize=(12,4))
# PartialDependenceDisplay.from_estimator(model, feature_names=X.columns,
#     features=['Open', 'Close'],
#     X=X, grid_resolution=50, ax=ax);

# ax.set_title('BitcoinR&F')

# plt.show()

#draw conclusions on the the graphs significance

# !pip install shap

# #shapley values
# #import shap

# # instantiate
# from xgboost import XGBClassifier
# xgb = XGBClassifier(random_state=42)
# model = xgb.fit(X, y)

# # Shap explainer initilization
# shap_ex = shap.TreeExplainer(model)

# # Determine Shap values
# shap_values = shap_ex.shap_values(X)
# # Shap explainer initilization
# shap_ex = shap.TreeExplainer(model)

# # Determine Shap values
# shap_values = shap_ex.shap_values(X_train)

# # Plot Shap values
# #shap.summary_plot(shap_values, X_train)

# #inistialize the plot with shap.initjs()
# shap.initjs()

# shap.force_plot(shap_ex.expected_value, shap_values[25,:], X.iloc[25,:])

#use matplot for plotting the shapley values

# # Calculate the shapley values
# shap_values = shap_ex.shap_values(X)

# # Initialize the plot
# shap.initjs()

# shap.force_plot(shap_ex.expected_value, shap_values[25,:], X.iloc[25,:])

